import pandas as pd
import numpy as np
import statsmodels.api as sm
import time
from datetime import datetime
import logging
from tqdm import tqdm
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import glob
import folium
from folium.plugins import MarkerCluster

# ë¡œê·¸ ì„¤ì •
logging.basicConfig(filename='log_contribution_cluster.txt', level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s')

print("â± ì‹œì‘ ì‹œê°:", datetime.now().strftime("%H:%M:%S"))
start_all = time.time()

# ğŸ“ íŒŒì¼ ê²½ë¡œ ì„¤ì •
positive_files = glob.glob("result_chunks/results_part_*.csv")
negative_files = glob.glob("random_chunks/negative_part_*.csv")

# ğŸ”„ ë°ì´í„° ë³‘í•©
dfs = []
print(f"ğŸ“‚ ë¯¼ì› ë°œìƒ íŒŒì¼ {len(positive_files)}ê°œ ë³‘í•© ì¤‘...")
for file in tqdm(positive_files, desc="Positive"):
    try:
        df = pd.read_csv(file)
        dfs.append(df)
    except Exception as e:
        logging.error(f"íŒŒì¼ {file} ì½ê¸° ì‹¤íŒ¨: {e}")

print(f"ğŸ“‚ ë¯¼ì› ì—†ìŒ íŒŒì¼ {len(negative_files)}ê°œ ë³‘í•© ì¤‘...")
for file in tqdm(negative_files, desc="Negative"):
    try:
        df = pd.read_csv(file)
        dfs.append(df)
    except Exception as e:
        logging.error(f"íŒŒì¼ {file} ì½ê¸° ì‹¤íŒ¨: {e}")

# ğŸ”— ì „ì²´ ë³‘í•©
full_df = pd.concat(dfs, ignore_index=True)
print(f"âœ… ì´ ë°ì´í„° ìˆ˜: {len(full_df)}")

# ğŸ§¹ ê²°ì¸¡ê°’ ì œê±°
clean_df = full_df.dropna(subset=['ì´ì£¼ì°¨ë©´ìˆ˜', 'í‰ê· ìš”ê¸ˆ', 'í‰ê· ìš´ì˜ì‹œê°„', 'CCTVê°œìˆ˜', 'ë¯¼ì›ë°œìƒ', 'ìœ„ë„', 'ê²½ë„'])

# ğŸ§© interaction terms ìƒì„±
clean_df['ì´ì£¼ì°¨ë©´ìˆ˜_ìš´ì˜ì‹œê°„'] = clean_df['ì´ì£¼ì°¨ë©´ìˆ˜'] * clean_df['í‰ê· ìš´ì˜ì‹œê°„']
clean_df['í‰ê· ìš”ê¸ˆ_ìš´ì˜ì‹œê°„'] = clean_df['í‰ê· ìš”ê¸ˆ'] * clean_df['í‰ê· ìš´ì˜ì‹œê°„']
clean_df['ì´ì£¼ì°¨ë©´ìˆ˜_í‰ê· ìš”ê¸ˆ'] = clean_df['ì´ì£¼ì°¨ë©´ìˆ˜'] * clean_df['í‰ê· ìš”ê¸ˆ']

# ğŸ¯ ë¡œì§€ìŠ¤í‹± íšŒê·€ ì „ì²˜ë¦¬
feature_cols = [
    'ì´ì£¼ì°¨ë©´ìˆ˜', 'í‰ê· ìš”ê¸ˆ', 'í‰ê· ìš´ì˜ì‹œê°„', 'CCTVê°œìˆ˜',
    'ì´ì£¼ì°¨ë©´ìˆ˜_ìš´ì˜ì‹œê°„', 'í‰ê· ìš”ê¸ˆ_ìš´ì˜ì‹œê°„', 'ì´ì£¼ì°¨ë©´ìˆ˜_í‰ê· ìš”ê¸ˆ'
]
X_raw = clean_df[feature_cols]
y = clean_df['ë¯¼ì›ë°œìƒ']
scaler = StandardScaler()
X_scaled = pd.DataFrame(scaler.fit_transform(X_raw), columns=X_raw.columns)
X_scaled = sm.add_constant(X_scaled)

print("ğŸ” ë¡œì§€ìŠ¤í‹± íšŒê·€ ë¶„ì„ ì¤‘...")
try:
    model = sm.Logit(y, X_scaled).fit()
    print(model.summary())
except Exception as e:
    print(f"âš ï¸ íšŒê·€ ë¶„ì„ ì‹¤íŒ¨: {e}")
    logging.error(f"íšŒê·€ ë¶„ì„ ì‹¤íŒ¨: {e}")
    exit()

# âœ… ê³„ìˆ˜ ì¶”ì¶œ (ìƒìˆ˜ ì œì™¸)
coef = model.params.drop('const')
print("ğŸ“Œ íšŒê·€ ê³„ìˆ˜:", coef.to_dict())

# ğŸ’¡ ê¸°ì—¬ë„ ê³„ì‚° (ë¯¼ì›ë°œìƒ == 1ì¸ ì§€ì ë§Œ)
positive_only = clean_df[clean_df['ë¯¼ì›ë°œìƒ'] == 1].copy()
X_pos_raw = positive_only[feature_cols]
X_pos_scaled = pd.DataFrame(scaler.transform(X_pos_raw), columns=feature_cols)

contribs = []
print("ğŸ§® ê¸°ì—¬ë„ ê³„ì‚° ì¤‘...")
for idx, row in tqdm(X_pos_scaled.iterrows(), total=len(X_pos_scaled)):
    contrib = {}
    for var in coef.index:
        contrib[f"ê¸°ì—¬ë„_{var}"] = coef[var] * row[var]
    contrib['ìœ„ë„'] = positive_only.iloc[idx]['ìœ„ë„']
    contrib['ê²½ë„'] = positive_only.iloc[idx]['ê²½ë„']
    contribs.append(contrib)

contrib_df = pd.DataFrame(contribs)
df_with_contrib = pd.concat([positive_only.reset_index(drop=True), contrib_df], axis=1)

# ğŸ”— í´ëŸ¬ìŠ¤í„°ë§
contrib_only = contrib_df[[col for col in contrib_df.columns if col.startswith("ê¸°ì—¬ë„_")]]
kmeans = KMeans(n_clusters=3, random_state=42)
df_with_contrib['í´ëŸ¬ìŠ¤í„°'] = kmeans.fit_predict(contrib_only)

# ğŸ’¾ ì €ì¥
df_with_contrib.to_csv("clustered_contribution_with_interaction.csv", index=False)
print("âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: clustered_contribution_with_interaction.csv")

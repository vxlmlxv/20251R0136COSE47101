import pandas as pd
import glob
import statsmodels.api as sm
import time
from datetime import datetime
import logging
from tqdm import tqdm
from sklearn.cluster import KMeans

# ë¡œê·¸ ì„¤ì •
logging.basicConfig(filename='log_contribution_cluster.txt', level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s')

print("â± ì‹œì‘ ì‹œê°:", datetime.now().strftime("%H:%M:%S"))
start_all = time.time()

# ğŸ“ íŒŒì¼ ê²½ë¡œ ì„¤ì •
positive_files = glob.glob("results_chunks/results_part_*.csv")
negative_files = glob.glob("random_chunks/negative_part_*.csv")

# ğŸ”„ ë°ì´í„° ë³‘í•©
dfs = []

print(f"ğŸ“‚ ë¯¼ì› ë°œìƒ íŒŒì¼ {len(positive_files)}ê°œ ë³‘í•© ì¤‘...")
for file in tqdm(positive_files, desc="Positive"):
    try:
        df = pd.read_csv(file)
        dfs.append(df)
    except Exception as e:
        logging.error(f"íŒŒì¼ {file} ì½ê¸° ì‹¤íŒ¨: {e}")

print(f"ğŸ“‚ ë¯¼ì› ì—†ìŒ íŒŒì¼ {len(negative_files)}ê°œ ë³‘í•© ì¤‘...")
for file in tqdm(negative_files, desc="Negative"):
    try:
        df = pd.read_csv(file)
        dfs.append(df)
    except Exception as e:
        logging.error(f"íŒŒì¼ {file} ì½ê¸° ì‹¤íŒ¨: {e}")

# ğŸ”— ì „ì²´ ë³‘í•©
full_df = pd.concat(dfs, ignore_index=True)
print(f"âœ… ì´ ë°ì´í„° ìˆ˜: {len(full_df)}")

# ğŸ§¹ ê²°ì¸¡ê°’ ì œê±°
clean_df = full_df.dropna()

# ğŸ¯ ë¡œì§€ìŠ¤í‹± íšŒê·€
X = clean_df[['ì´ì£¼ì°¨ë©´ìˆ˜', 'í‰ê· ìš”ê¸ˆ', 'í‰ê· ìš´ì˜ì‹œê°„', 'CCTVê°œìˆ˜']]
y = clean_df['ë¯¼ì›ë°œìƒ']
X = sm.add_constant(X)

print("ğŸ” ë¡œì§€ìŠ¤í‹± íšŒê·€ ë¶„ì„ ì¤‘...")
try:
    model = sm.Logit(y, X).fit()
    print(model.summary())
except Exception as e:
    print(f"âš ï¸ íšŒê·€ ë¶„ì„ ì‹¤íŒ¨: {e}")
    logging.error(f"íšŒê·€ ë¶„ì„ ì‹¤íŒ¨: {e}")
    exit()

# âœ… ê³„ìˆ˜ ì¶”ì¶œ (ìƒìˆ˜ ì œì™¸)
coef = model.params.drop('const')
print("ğŸ“Œ íšŒê·€ ê³„ìˆ˜:", coef.to_dict())

# ğŸ’¡ ê¸°ì—¬ë„ ê³„ì‚° (ë¯¼ì›ë°œìƒ == 1ì¸ ì§€ì ë§Œ)
positive_only = clean_df[clean_df['ë¯¼ì›ë°œìƒ'] == 1].copy()

contribs = []
print("ğŸ§® ê¸°ì—¬ë„ ê³„ì‚° ì¤‘...")
for idx, row in tqdm(positive_only.iterrows(), total=len(positive_only)):
    contrib = {}
    for var in coef.index:
        contrib[f"ê¸°ì—¬ë„_{var}"] = coef[var] * row[var]
    contribs.append(contrib)

contrib_df = pd.DataFrame(contribs)
df_with_contrib = pd.concat([positive_only.reset_index(drop=True), contrib_df], axis=1)

# ğŸ“Š í´ëŸ¬ìŠ¤í„°ë§ (ê¸°ì—¬ë„ ê¸°ì¤€)
kmeans = KMeans(n_clusters=3, random_state=42)
df_with_contrib['í´ëŸ¬ìŠ¤í„°'] = kmeans.fit_predict(contrib_df)

# ğŸ’¾ ì €ì¥
output_file = "clustered_contribution_from_chunks.csv"
df_with_contrib.to_csv(output_file, index=False)
print(f"âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_file}")

end_all = time.time()
print("â± ì¢…ë£Œ ì‹œê°:", datetime.now().strftime("%H:%M:%S"))
print(f"â± ì „ì²´ ì†Œìš” ì‹œê°„: {end_all - start_all:.2f}ì´ˆ")
